# -*- coding: utf-8 -*-
"""sketch_to_image_translation_using_GANs.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1FNeyka-p_F4vFHbOYIZfxATqy1ivaILE

**Importing the required DL libraries**
"""

from keras.layers import (
    Input,Dense,Reshape,Flatten,Dropout,Concatenate)
from keras.layers import BatchNormalization, Activation, ZeroPadding2D
from keras.layers.advanced_activations import LeakyReLU
from keras.layers.convolutional import UpSampling2D, Conv2D,Conv2DTranspose
from keras.models import  Model

import tensorflow
from tensorflow.keras.optimizers import Adam

"""**Mounting Google Drive since the Dataset is stored in it, named as 'V'**"""

from google.colab import drive
drive.mount('/content/drive')

import cv2
from glob import glob
import numpy as np
import matplotlib.pyplot as plt

dataset_name ='V'

"""**Using Adam optimizer**"""

#keras.optimizers.Adam(lr=0.0002, beta_1=0.9, beta_2=0.999, epsilon=None, decay=0.0, amsgrad=False)
optimizer=Adam(0.0002,0.5)

"""[ADAM](https://qph.fs.quoracdn.net/main-qimg-e2d7c3670264380c730bed6de491e10f)

![alt text](https://image.slidesharecdn.com/20171017dlhacks-171019082642/95/dlhacks-perceptual-adversarial-networks-for-imagetoimage-transformation-7-638.jpg?cb=1508401655)
"""

from tensorflow.keras.optimizers import(Adam, RMSprop)

from keras.layers import( Conv2D, UpSampling2D)

from keras.models import Model

"""**specifying the image size and other details**"""

img_row=256
img_col = 256
channels =3
img_shape=(img_row,img_col,channels)
patch= int(img_row/2**4)
disc_patch=(patch,patch,1)
dwf=64
upf =64
img_A= Input(shape=(256,256,3))
img_B =Input(shape=(256,256,3))

"""**Generator Architecture**

![alt text](https://cdn-images-1.medium.com/max/1600/1*TktHhY3l89quwJtCtVg86Q.png)

**Encoder**
"""

def encode(inputs,filters,kernel=4,bn=True):
  d=Conv2D(filters=filters,kernel_size=kernel,strides=2,padding='same')(inputs)
  d=LeakyReLU(alpha=0.2)(d)
  if bn:
    d= BatchNormalization(momentum=0.8)(d)
    
  return d

#down filter-> dwf=64 upfilter->upf=64

"""**Decoder**"""

def decode(layer_input, skip_input, filters, f_size=4, dropout_rate=0):
            """Layers used during upsampling"""
            u = UpSampling2D(size=2)(layer_input)
            #u = Conv2D(filters, kernel_size=f_size, strides=1, padding='same', activation='relu')(u)
            u = Conv2DTranspose(filters, kernel_size=f_size, strides=1, padding='same', activation='relu')(u)
            if dropout_rate:
                u = Dropout(dropout_rate)(u)
            u = BatchNormalization(momentum=0.8)(u)
            u = Concatenate()([u, skip_input])
            return u

"""**Generator Architecture**"""

def generator():
  inputs=Input(shape=(256,256,3))
  d0=encode(inputs,dwf,bn=False)
  d1=encode(d0,dwf*2)
  d2=encode(d1,dwf*4)
  d3=encode(d2,dwf*8)
  d4=encode(d3,dwf*8)
  d5=encode(d4,dwf*8)
  d6=encode(d5,dwf*8)
  ## upsampling##
  u1=decode(d6,d5,upf)
  u2=decode(u1,d4,upf*8)
  u3=decode(u2,d3,upf*8)
  u4=decode(u3,d2,upf*8)
  u5=decode(u4,d1,upf*4)
  u6=decode(u5,d0,upf*2)
  u7=UpSampling2D(size=2)(u6)
  out = Conv2D(channels,kernel_size=4,strides=1,padding='same',activation='tanh' )(u7)
  return Model(inputs,out)

G=generator()

G.summary()
# Input images and their conditioning images
img_A = Input(shape=img_shape)
img_B = Input(shape=img_shape)

"""**Discriminator Architecture**

![alt text](https://cdn-images-1.medium.com/max/1600/1*O_8exW8NYPmTLjJujy-wWg.png)
"""

def descriminator():
  combined_imgs = Concatenate(axis=-1)([img_A, img_B])
  d1=encode(combined_imgs,dwf,bn=False)
  d2=encode(d1,dwf*2)
  d3=encode(d2,dwf*4)
  d4=encode(d3,dwf*8)
  validity = Conv2D(1, kernel_size=4, strides=1, padding='same')(d4)
  return Model([img_A, img_B], validity)

D=descriminator()
D.summary()

"""**Finding loss and accuracy**"""

D.compile(loss='mse',
            optimizer=optimizer,
            metrics=['accuracy'])
# By conditioning on B generate a fake version of A
fake_A = G(img_B)
 # Discriminators determines validity of  condition pairs
valid = D([fake_A, img_B])

#combined model LAmbda =100
combined = Model(inputs=[img_A, img_B], outputs=[valid, fake_A])
combined.compile(loss=['mse', 'mae'],
                              loss_weights=[1, 100],
                              optimizer=optimizer)

#print(fake_A.shape)
# For the combined model we will only train the generator
D.trainable = False
n_batches= 1;

#image resolution for resizing
img_res=(256,256)
def imread( path):
        cv2.imread(path)

"""**Saving the model weights inside a directory saved_model**"""

!mkdir saved_model

"""**Loading Dataset**"""

##load datasets

def load_data( batch_size=1, is_testing=False):
        data_type = "train" if not is_testing else "test"
        path = glob('/content/drive/My Drive/%s/%s/*' % (dataset_name, data_type))

        batch_images = np.random.choice(path, size=batch_size)

        imgs_A = []
        imgs_B = []
        for img_path in batch_images:
            img = cv2.imread(img_path)

            h, w, _ = img.shape
            _w = int(w/2)
            img_A, img_B = img[:, :_w, :], img[:, _w:, :]

            img_A = cv2.resize(img_A, img_res)
            img_B = cv2.resize(img_B, img_res)

            # If training => do random flip
            if not is_testing and np.random.random() < 0.5:
                img_A = np.fliplr(img_A)
                img_B = np.fliplr(img_B)

            imgs_A.append(img_A)
            imgs_B.append(img_B)

        imgs_A = np.array(imgs_A)/127.5 - 1.
        imgs_B = np.array(imgs_B)/127.5 - 1.

        return imgs_A, imgs_B

"""**Loading batch**"""

#load batch
def load_batch(batch_size=1, is_testing=False):
        data_type = "train" if not is_testing else "val"
        path = glob('/content/drive/My Drive/%s/%s/*' % (dataset_name, data_type))

        n_batches = int(len(path) / batch_size)

        for i in range(n_batches-1):
            batch = path[i*batch_size:(i+1)*batch_size]
            imgs_A, imgs_B = [], []
            for img in batch:
                img = cv2.imread(img)
                h, w, _ = img.shape
                half_w = int(w/2)
                img_A = img[:, :half_w, :]
                img_B = img[:, half_w:, :]

                img_A = cv2.resize(img_A, img_res)
                img_B = cv2.resize(img_B, img_res)

                if not is_testing and np.random.random() > 0.5:
                        img_A = np.fliplr(img_A)
                        img_B = np.fliplr(img_B)

                imgs_A.append(img_A)
                imgs_B.append(img_B)

            imgs_A = np.array(imgs_A)/127.5 - 1.
            imgs_B = np.array(imgs_B)/127.5 - 1.

            yield imgs_A, imgs_B

def sample_images(epoch, batch_i):
       
        r, c = 3, 3

        imgs_A, imgs_B = load_data(batch_size=3, is_testing=True)
        fake_A = G.predict(imgs_B)

        gen_imgs = np.concatenate([imgs_B, fake_A, imgs_A])

        # Rescale images 0 - 1
        gen_imgs = 0.5 * gen_imgs + 0.5

        titles = ['Condition', 'Generated', 'Original']
        fig, axs = plt.subplots(r, c)
        cnt = 0
        for i in range(r):
            for j in range(c):
                axs[i,j].imshow(gen_imgs[cnt])
                axs[i, j].set_title(titles[i])
                axs[i,j].axis('off')
                cnt += 1
        plt.show()
        plt.close()

"""**Training**"""

import datetime
import os
def train(epochs, batch_size=1, sample_interval=50):

        start_time = datetime.datetime.now()

        # Adversarial loss ground truths
        valid = np.ones((batch_size,) + disc_patch)
        fake = np.zeros((batch_size,) + disc_patch)
        print(valid.shape)

        for epoch in range(epochs):
            for batch_i, (imgs_A, imgs_B) in enumerate(load_batch(batch_size)):

                # ---------------------
                #  Train Discriminator
                # ---------------------

                # Condition on B and generate a translated version
                fake_A = G.predict(imgs_B)
                #print(imgs_A.shape)

                # Train the discriminators (original images = real / generated = Fake)
                d_loss_real = D.train_on_batch([imgs_A, imgs_B], valid)
                d_loss_fake = D.train_on_batch([fake_A, imgs_B], fake)
                d_loss = 0.5 * np.add(d_loss_real, d_loss_fake)

                # -----------------
                #  Train Generator
                # -----------------

                # Train the generators
                g_loss = combined.train_on_batch([imgs_A, imgs_B], [valid, imgs_A])

                elapsed_time = datetime.datetime.now() - start_time
                # Plot the progress
                print ("[Epoch %d/%d] [Batch %d] [D loss: %f, acc: %3d%%] [G loss: %f] time: %s" % (epoch, epochs,
                                                                        batch_i,
                                                                        d_loss[0], 100*d_loss[1],
                                                                        g_loss[0],
                                                                        elapsed_time))

                # If at save interval => save generated image samples
                if batch_i % sample_interval == 0:
                    sample_images(epoch, batch_i)
                    gen_weights_path = os.path.join('./saved_model/gen_weights_epoch_%s.h5' % (epoch))
                    G.save_weights(gen_weights_path, overwrite=True)

train(200,3,20)